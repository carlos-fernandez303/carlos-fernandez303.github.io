{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://www.incimages.com/uploaded_files/image/1920x1080/getty_539884497_2000133320009280223_219746.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Profanity in Hip Hop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carlos Fernandez\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrey Knyazev "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markus Ferrell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "#### \\**Please note. Though we censored the words in the dataset along with our prose, the primary focus of this project is explicit language.\\**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is truly a feat how much hip hop has changed since the early days of DJ Kool Herc and MC Coke La Rock laying the foundations for what would become one of the most popular music genre's in the world. A common theme that could be found in today's hip hip scene would be varying degrees of profanity. Anything from the N-word to the F-bomb could likely be heard at least once in your average billboard entry. Ultimately, we wanted to what kind of relationships we can unravel with this profanity dataset using different processing methods within the realm of data science.\n",
    "\n",
    "\n",
    "With the help of data science, we will examine the prevalence of profanity in popular hip hop over a span of time. In this case, the most 5 most popular albums for each year from 1985 to 2018 will be analyzed. These albums in the dataset were chosen based on total sales and artist popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are concerns regarding the effects of an increase in profanity in popular\n",
    "hip-hop/rap songs, particularly pertaining to youth. Adults for the most part are unaffected by an\n",
    "increase or decrease in profanity in popular rap music, however youth that are susceptible to\n",
    "being easily influenced by these lyrics. Youth are very impressionable, and when popular music\n",
    "contains frequent profanity that depicts violence, racism, homophobia or sexism has been\n",
    "shown to have a correlation with teens and risky behaviors as a result of them being fans of the\n",
    "hip-hop/rap genre. This is mostly prevalent in sexual health and behavior, as well as alcohol and\n",
    "drug usage.\n",
    "\n",
    "Here is an npr article referencing how rap music is linked to alcohol and violence:\n",
    "https://www.npr.org/templates/story/story.php?storyId=5390075\n",
    "\n",
    "We also have a study where 500 college students are exposed to violent music and then\n",
    "perform word-association tasks and their subsequent conclusion of an existing correlation and\n",
    "causation:\n",
    "\n",
    "https://www.apa.org/monitor/julaug03/violent\n",
    "\n",
    "Our analysis shows this increase in profanity in popular hip-hop/rap music has been a\n",
    "steady trend over the past 33 years and this upward trend is likely to continue in coming years.\n",
    "In conjunction with two of many studies, we establish the effect of popular rap music and the\n",
    "amount of profanity it contains on the impressionable demographic of a population, particularly\n",
    "the youth. This can develop into a potential issue within a society, and our data supports these\n",
    "conclusions. As you read through our analysis of the usage of profanity in popular hip-hop/rap\n",
    "music, please keep these trends and correlations in mind and consider the potential effects they\n",
    "may have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Python libraries were used to explore, manipulate and analyze the profanity dataset: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import math\n",
    "from ggplot import *\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load the .csv (comma-separated value) file which will produce the Pandas DataFrame with the hip hop profanity data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Profanity in Hip Hop.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-8a9fef7961ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhiphop_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Profanity in Hip Hop.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mhiphop_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1168\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1169\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1998\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Profanity in Hip Hop.csv'"
     ]
    }
   ],
   "source": [
    "hiphop_df = pd.read_csv('Profanity in Hip Hop.csv')\n",
    "hiphop_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas DataFrame above displays the profanity information for every single album that was selected for this study. The 'Swear Word' column is what we are all here for. Each album and their respective tracks were examined for the occurrences of 11 common profanity words.\n",
    "\n",
    "The selected profanity words comprise of:\n",
    "\n",
    "1. F**k\n",
    "2. S**t\n",
    "3. B***h\n",
    "4. P***y \n",
    "5. H*e\n",
    "6. N-Word\n",
    "7. D**k\n",
    "8. C**k\n",
    "9. D**k\n",
    "10. T**s\n",
    "11. A*s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidying the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we actually process this data and make some exploratory analyses, we will brush up the data.\n",
    "\n",
    "First, we would like to censor the swear words in the 'Swear Word' column of our Pandas DataFrame. Even though examining the prevalence of profanity words in Hip Hop is a unique and interesting topic, it is unnecessary and perhaps even distracting to leave the profanity words uncensored, especially when curating our message covering insights learned in this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in hiphop_df.iterrows():\n",
    "    if row['Swear Word'] == 'Fuck':\n",
    "        hiphop_df.at[index, 'Swear Word'] = 'F**k'\n",
    "    elif row['Swear Word'] == 'Shit':\n",
    "        hiphop_df.at[index, 'Swear Word'] = 'S**t'\n",
    "    elif row['Swear Word'] == 'Bitch':\n",
    "        hiphop_df.at[index, 'Swear Word'] = 'B**h'\n",
    "    elif row['Swear Word'] == 'Pussy':\n",
    "        hiphop_df.at[index, 'Swear Word'] = 'P***y'     \n",
    "    elif row['Swear Word'] == 'Dick':\n",
    "        hiphop_df.at[index, 'Swear Word'] = 'D**k'\n",
    "    elif row['Swear Word'] == 'Cock':\n",
    "        hiphop_df.at[index, 'Swear Word'] = 'C**k'\n",
    "    elif row['Swear Word'] == 'Tits':\n",
    "        hiphop_df.at[index, 'Swear Word'] = 'T**s'\n",
    "\n",
    "        \n",
    "hiphop_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this DataFrame has the most profane words censored in its 'Swear Words' column. Furthermore, the DataFrame itself has a few columns that contain repetitive information on the artists. \n",
    "\n",
    "The following code will alter the order of the DataFrame columns along with dropping unnecessary columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiphop_df = hiphop_df.drop(columns=['Artist - Album', 'Artist - Song', 'Spotify URI'])\n",
    "hiphop_df = hiphop_df[['Year','Artist', 'Song & Featuring', 'Album', 'Song', 'Swear word in Title?', 'Swear Word', 'Times Used', 'Released as a Single?', 'Single - Highest US Charting Position']]\n",
    "hiphop_df.tail(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data processing purposes, we will change the NaN value in the 'Times Used' Column to the floating-point value 0.0.\n",
    "\n",
    "For the last step in the tidying process, we want to isolate the featured artist(s) from all of the entries in the 'Song & Featuring' column because we do not need the song title included when we already have a column for it. \n",
    "\n",
    "This will include extracting the string from each 'Song & Featuring', removing the song name itself, and wrapping it up by renaming the column to reflect the newly tidied DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code utilizes some RegEx functions to search a string for a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in hiphop_df.iterrows():\n",
    "    \n",
    "    if (math.isnan(row['Times Used'])):\n",
    "        hiphop_df.at[index, 'Times Used'] = 0.0\n",
    "    \n",
    "    match = re.search(\"\\(featuring.+\\)\", row['Song & Featuring']) \n",
    "    if (match):\n",
    "        hiphop_df.at[index, 'Song & Featuring'] = match.group()[10:-1]\n",
    "    else:\n",
    "        hiphop_df.at[index, 'Song & Featuring'] = '-'\n",
    "        \n",
    "\n",
    "hiphop_df = hiphop_df.rename(columns = {'Song & Featuring': 'Featuring Artist'})\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "       \n",
    "\n",
    "                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiphop_df[29142:29147]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br>\n",
    "\n",
    "## Exploratory Data Analysis / Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>\n",
    "#### Profanity Over Time \n",
    "\n",
    "We will begin with the most obvious possible trend out of the data. This has been mentioned over and over in media and in different social circles. This would be the trend of rap becomming more vulgar over time. To do this, we are plotting the usage of swear words over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply analyzing it as swear usage over time with labels to help distinguish the words used\n",
    "\n",
    "plt.scatter(hiphop_df['Year'], hiphop_df['Times Used'], s = 3)\n",
    "\n",
    "X = hiphop_df['Year'].values.reshape(-1,1)\n",
    "Y = hiphop_df['Times Used'].replace(np.nan, 0)\n",
    "#making the model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X,Y)\n",
    "    \n",
    "#creating the line based on the model and plotting\n",
    "prediction = reg.predict(X)\n",
    "plt.plot(X, prediction, label = 'Linear Regression')\n",
    "\n",
    "\n",
    "\n",
    "plt.title('Times Used vs Year')\n",
    "plt.ylabel('Times Used')\n",
    "plt.xlabel('Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking about this code though, it does not really answer the question that we are searching for. Instead, we should make a graph about the number of swears in any particular rap song per year. In this way, we can lump all of the swears into one profanity category. Doing so will allow us to see if songs are getting more profane over time. This is a far more understandable piece of data. <br></br><br></br><br></br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = hiphop_df.groupby(['Artist', 'Song'])\n",
    "\n",
    "# creating a dataframe which has the sum of all of the swears rather than the individual swear's sums\n",
    "songs_df = pd.DataFrame()\n",
    "for group_name, group in groups:\n",
    "    row = group.iloc[0]\n",
    "    songs_df = songs_df.append({'Artist': row['Artist'],'Song': row['Song'] ,'Year': row['Year'],'SongSum': np.sum(group['Times Used']), 'Single - Highest US Charting Position': row['Single - Highest US Charting Position']}, ignore_index = True)\n",
    "    \n",
    "songs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(songs_df['Year'], songs_df['SongSum'], s = 3)\n",
    "\n",
    "X = songs_df['Year'].values.reshape(-1,1)\n",
    "Y = songs_df['SongSum'].replace(np.nan, 0)\n",
    "#making the model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X,Y)\n",
    "    \n",
    "#creating the line based on the model and plotting\n",
    "prediction = reg.predict(X)\n",
    "plt.plot(X, prediction, label = 'Linear Regression')\n",
    "\n",
    "\n",
    "plt.title('Swears Used Per Song vs Year')\n",
    "plt.ylabel('Swears Used Per Song')\n",
    "plt.xlabel('Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In creating this graph, we see that the trend is far more apparent. The average number of curse words is relatively high per song and it has been increasing gradually over time.\n",
    "\n",
    "This graph makes the assumption that every swear is weighted the same. This might not be the case for listeners however. For example, listeners may think that the F-word or the N-word are far more profane than the A-word. In order to better understand the data and the trends of rap profanity over time, we must break these individual words apart. That is what we will do with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swear_groups = hiphop_df.groupby('Swear Word')\n",
    "label = []\n",
    "\n",
    "print('Slopes for each swear\\'s regression line')\n",
    "\n",
    "\n",
    "for swear, group in swear_groups:\n",
    "    label.append(swear)\n",
    "    plt.scatter(group['Year'], group['Times Used'], s = 3)\n",
    "    \n",
    "    X = group['Year'].values.reshape(-1,1)\n",
    "    Y = group['Times Used'].replace(np.nan, 0)\n",
    "    #making the model\n",
    "    reg = linear_model.LinearRegression()\n",
    "    reg.fit(X,Y)\n",
    "    \n",
    "    #creating the line based on the model and plotting\n",
    "    prediction = reg.predict(X)\n",
    "    plt.plot(X, prediction, label = 'Linear Regression')\n",
    "    print(swear, \": \", reg.coef_)\n",
    "    \n",
    "    \n",
    "        \n",
    "plt.legend(label, bbox_to_anchor = (1.05, 1), loc = 'upper left')\n",
    "plt.title('Times Used vs Year')\n",
    "plt.ylabel('Times Used')\n",
    "plt.xlabel('Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the trends of individual words are radically different. Words like the B-word, S-word, and N-word are all increasing in usage at far higher rates than other words. In contrast, words such as the A-word, C-word, and D-word are gradually decreasing in usage.\n",
    "\n",
    "Another important note of this data is the position of the regression lines. We can see that the regression lines are close to the bottom of the graph which means there is a noteable number of 0's for each word that are dragging it down. Perhaps, another way to analyze this dataset is to check if cleanliness is decreasing rather than if vulgarity is increasing.\n",
    "\n",
    "To do so, we will first look and see the number of songs that have no vulgarity in them over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = songs_df.groupby(['Year', 'Song'])\n",
    "\n",
    "years = np.array([])\n",
    "sums = []\n",
    "\n",
    "for group_name, groups in groups:\n",
    "    \n",
    "    years = np.append(years, group_name[0])\n",
    "    sums.append(len(groups.index))\n",
    "    \n",
    "    \n",
    "\n",
    "plt.scatter(years, sums)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Clean Songs\")\n",
    "plt.title(\"Number of Clean Songs vs Time\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little unsurprisingly, there were little to no completely clean songs throughout the entire dataset that we had. Out of the whole dataset, we have 10 songs that are completely clean. We must take a different approach to assessing cleanliness.\n",
    "\n",
    "The way we will assess cleanliness now is to take the number of swear word categories that have nothing in them and divide them by the number of overall swear word categories for that year. This will give us the ratio of cleanliness of the songs. For example, if the song does not contain 8 out of the 11 curse words, it will be 8/11 clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiphop_df1 = pd.read_csv('Profanity in Hip Hop.csv')\n",
    "\n",
    "culled_df = hiphop_df1.dropna(subset=['Times Used'])\n",
    "zeros_df = pd.concat([hiphop_df1, culled_df]).drop_duplicates(keep = False)\n",
    "\n",
    "years = np.array([])\n",
    "sums = []\n",
    "groups = zeros_df.groupby(['Year'])\n",
    "for group_name, groups in groups:\n",
    "        \n",
    "    years = np.append(years, group_name)\n",
    "    sums.append(len(groups.index)/len(hiphop_df.loc[hiphop_df['Year'] == group_name]))\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "X = years.reshape(-1,1)\n",
    "Y = sums\n",
    "#making the model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X, Y)\n",
    "    \n",
    "#creating the line based on the model and plotting\n",
    "prediction = reg.predict(X)\n",
    "plt.plot(X, prediction, label = 'Linear Regression')\n",
    "\n",
    "plt.plot(years, sums)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Ratio of Words Unused over Used in songs\")\n",
    "plt.title(\"Cleanliness of Songs vs Time\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Slope\", reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My making this standard, we see that originally, rap music was largely clean and did not use many different profane words. As time progresses though, the ratio of cleanliness decreases. It reached a low around 2001 and then came back up to a steady point a few years later. The decrease that we see from our linear regression is a slope of -0.4% each year in cleanliness. Though this doesn't seem like a lot, over many years it has a clear effect on the genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "billboard_df = songs_df.dropna(subset = ['Single - Highest US Charting Position'])\n",
    "\n",
    "\n",
    "X = billboard_df['SongSum'].values.reshape(-1,1)\n",
    "Y = billboard_df['Single - Highest US Charting Position']\n",
    "#making the model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X, Y)\n",
    "    \n",
    "#creating the line based on the model and plotting\n",
    "prediction = reg.predict(X)\n",
    "plt.plot(X, prediction, label = 'Linear Regression')\n",
    "\n",
    "\n",
    "plt.scatter(billboard_df['SongSum'], billboard_df['Single - Highest US Charting Position'])\n",
    "plt.xlabel('Number of Swears')\n",
    "plt.ylabel('US Charting Position')\n",
    "plt.title('US Charting Postion vs Number of Swears')\n",
    "plt.show()\n",
    "print('Slope: ', reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying regression, we are able to find that the trend has a slope of -0.176. This means that as the number of swears increases, the US charting position goes down. This is important because number 1 in US Charting position is ideal. In other words, the trend is pointing out that the increase in number of swear words leads to a better US charting position.\n",
    "\n",
    "Based on the above data and the trend that was captured, we will apply machine learning to dig deeper into the trend and see if it is possible to predict a song being popular by the swears that it uses. <br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br><br></br>\n",
    "\n",
    "* Now we will produce a plot that contains the average amount of profanity per year in hip hop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 12))\n",
    "\n",
    "avg_words = profanity_by_year.mean()\n",
    "sns.regplot(x = avg_words.index, y = avg_words.values, data = avg_words)\n",
    "\n",
    "plt.xlim(1984, 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first noticeable difference from the previous gaph is that the trend is not as apparent. It almost appears as if this data may misrepresentative; however, this is because the distribution of the data across each year. Outliers, such as a particular album not having little to no profanity could greatly affect overal plot distribution. In order to better interpret the average swear words per song rather than an ambigous scatter plot, we implement a linear regression model on the data. This allows us to model the relationship between two variables by fitting a linear equation to observed data. \n",
    "\n",
    "As a result, a clear relationship can be seen between the increase in average swear words per song over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br><br></br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The following plot displays the amount of curse words per year (up to 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plist = pd.read_csv('Profanity in hip Hop.csv')\n",
    "#Boxplot showing amount of curse words as time progresses\n",
    "\n",
    "plt.figure(figsize = (18, 5))\n",
    "sns.boxplot(x = 'Year', y = 'Times Used', data = plist, palette = \"Set2\")\n",
    "\n",
    "axes = plt.gca()\n",
    "plt.ylim(0,24)\n",
    "\n",
    "plt.title(\"Amount of curse words per year up to 25\")\n",
    "plt.xlabel(\"Times Used\")\n",
    "plt.ylabel(\"Year\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above data and trends, we are curious if we can build a model to predict a song's popularity based on the number of swears that it uses. With this model, we can assess them to build predictions on the overall rap scene.\n",
    "\n",
    "In order to get a machine learning model to work with the data, we must make some changes to the data.\n",
    "\n",
    "Firstly, since there is no metric for a song that did not get into the US Top charts, we will have a hole to fill. These N/A values need to be filled with something in order for the model to be able to assess correctly. We initially thought to fill it with some value and then the model would assess the number position a song would get in the top US songs. The issue with this procedure, however, is that there is no accurate number that we can give to it's position in the US top songs without skewing the data. Instead, what we are choosing to do is make it either 1 or 0. 1 being that they were in the top US songs and 0 being that they were not. This will allow the machine to accurately choose between them.\n",
    "\n",
    "The second thing we need to change is the data and it's placement in the table. For our model, we are interested in the possibility of predicting it being a top chart based on the number of swear words. In doing so, we must move the swear words to their own column. This will allow for a machine learning model to find trends between them. In addition, we think that the columns for whether the title contains a swear may also aid in the model. We will convert this column to a 1 or a 0 corresponding to the yes or no. Lastly we will include the year.\n",
    "\n",
    "We do not want to include releasing it as a single because this does not directly relate to the vulgarity of the song. Our model is trying to predict whether the vulgarity is an accurate measure for the popularity of the song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = hiphop_df.groupby(['Artist', 'Song'])\n",
    "\n",
    "# creating a dataframe which has the sum of all of the swears rather than the individual swear's sums\n",
    "ml_df = pd.DataFrame()\n",
    "\n",
    "for group_name, group in groups:\n",
    "    row = group.iloc[0]\n",
    "    f_rows = group.loc[group['Swear Word'] == 'F**k']\n",
    "    s_rows = group.loc[group['Swear Word'] == 'S**t']\n",
    "    b_rows = group.loc[group['Swear Word'] == 'B**ch']\n",
    "    p_rows = group.loc[group['Swear Word'] == 'P**sy']\n",
    "    d_rows = group.loc[group['Swear Word'] == 'D**k']\n",
    "    c_rows = group.loc[group['Swear Word'] == 'C**k']\n",
    "    t_rows = group.loc[group['Swear Word'] == 'T**s']\n",
    "    a_rows = group.loc[group['Swear Word'] == 'A*s']\n",
    "    h_rows = group.loc[group['Swear Word'] == 'H*e']\n",
    "    n_rows = group.loc[group['Swear Word'] == 'N-Word']\n",
    "    da_rows = group.loc[group['Swear Word'] == 'D**mn']\n",
    "    \n",
    "    f_val = np.sum(f_rows['Times Used'])\n",
    "    s_val = np.sum(s_rows['Times Used'])\n",
    "    b_val = np.sum(b_rows['Times Used'])\n",
    "    p_val = np.sum(p_rows['Times Used'])\n",
    "    d_val = np.sum(d_rows['Times Used'])\n",
    "    c_val = np.sum(c_rows['Times Used'])\n",
    "    t_val = np.sum(t_rows['Times Used'])\n",
    "    a_val = np.sum(a_rows['Times Used'])\n",
    "    h_val = np.sum(h_rows['Times Used'])\n",
    "    n_val = np.sum(n_rows['Times Used'])\n",
    "    da_val = np.sum(da_rows['Times Used'])\n",
    "\n",
    "    \n",
    "    \n",
    "    ml_df = ml_df.append({'Year': row['Year'],'F': f_val, 'S': s_val,'Bi': b_val, 'P': p_val, 'D': d_val,\n",
    "                          'C': c_val, 'T': t_val, 'A': a_val, 'H': h_val, 'N': n_val, 'Da': da_val,                \n",
    "                             'Highest US Charting Position': row['Single - Highest US Charting Position']},\n",
    "                            ignore_index = True)\n",
    "\n",
    "    \n",
    "ml_df = ml_df.replace(np.nan, -1)    \n",
    "ml_target = [0 if i==-1 else 1 for i in ml_df['Highest US Charting Position']]    \n",
    "ml_df = ml_df.drop(columns = ['Highest US Charting Position'])\n",
    "\n",
    "ml_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataframe is set up correctly, we can begin to build the model.\n",
    "\n",
    "For the model we are going to compare and contrast between SVC and k Nearest Neighbors to see if either can build an accurate model for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "n_scores = np.zeros(14)\n",
    "\n",
    "# testing c values for SVM fro m1-14 to determine which one produces the best model\n",
    "for i in range(1,15):\n",
    "    # set up the classifier\n",
    "    svm = SVC(kernel = 'linear', C = i)    \n",
    "    \n",
    "    # fits the classifier to the training data, tests it on remaining data designated for test data.\n",
    "    # It does this for each of the 10 regions of data when it is spit into 10 equal parts.\n",
    "    svm_scores = cross_val_score(svm, ml_df, ml_target, cv = 10)\n",
    "        \n",
    "    n_scores[i-1] = np.mean(svm_scores)\n",
    "\n",
    "plt.plot(range(1,15), n_scores)\n",
    "plt.xlabel(\"C Value\")\n",
    "plt.ylabel(\"Score (0-1)\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the SVM classifier\n",
    "# choosing the hyperparameter 1 because all of the sampled C values were the same\n",
    "svm = SVC(kernel = 'linear', C = 1)\n",
    "\n",
    "# fits the classifier to the training data, tests it on remaining data designated for test data.\n",
    "# It does this for each of the 10 regions of data when it is spit into 10 equal parts.\n",
    "svm_scores = cross_val_score(svm, ml_df, ml_target, cv = 10)\n",
    "\n",
    "# printing the average score from the CV testing\n",
    "print(\"Individual CV Scores: \", svm_scores)\n",
    "print(\"\\nCV Score: \", np.mean(svm_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "n_scores = np.zeros(20)\n",
    "\n",
    "# testing k values for KNN from 1-14 to determine which one is the best model\n",
    "for i in range(1,21):\n",
    "    # set up the classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors = i)\n",
    "    \n",
    "    # fits the classifier to the training data, tests it on remaining data designated for test data.\n",
    "    # It does this for each of the 10 regions of data when it is spit into 10 equal parts.\n",
    "    knn_scores = cross_val_score(knn, ml_df, ml_target, cv = 10)\n",
    "        \n",
    "    n_scores[i-1] = np.mean(knn_scores)\n",
    "\n",
    "plt.plot(range(1,21), n_scores)\n",
    "plt.xlabel(\"K Neighbors\")\n",
    "plt.ylabel(\"Score (0-1)\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the above graph, we can see that 14 is the highest value for k in this dataset.\n",
    "# I will apply this as our model\n",
    "\n",
    "# Create the 9-NN model\n",
    "knn = KNeighborsClassifier(n_neighbors = 14)\n",
    "\n",
    "# running 10 fold CV on the classifier\n",
    "knn_scores = cross_val_score(knn, ml_df, ml_target, cv = 10)\n",
    "print(\"Individual CV Scores: \", knn_scores)\n",
    "print(\"\\nCV Score: \", np.mean(knn_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(\"----- 10-FOLD CROSS VALIDATION STATISTICS -----\")\n",
    "print(\"\\nLinear SVM: \\nMean: \", np.mean(svm_scores), \"\\nStandard Deviation: \", np.std(svm_scores))\n",
    "print(\"Standard Error: \", stats.sem(svm_scores))\n",
    "print(\"\\nk-Nearest Neighbors: \\nMean: \", np.mean(knn_scores),\"\\nStandard Deviation: \", np.std(knn_scores))\n",
    "print(\"Standard Error: \", stats.sem(knn_scores))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--------------- RELATED T-TESTS ---------------\")\n",
    "print(\"\\nComparing k-NN with linear SVM: \\nP-Value: \", stats.ttest_rel(svm_scores, knn_scores).pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAN likely means a 0 pvalue. They are exactly the same model in every statistic so they should have a p-value of zero. Why it is specifically nan and not 0, we are not sure as the result for the t-test p-value is clearly 0 based on the two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we have learned that there has been an  undeniable increase in profanity in hip hop. \n",
    "\n",
    "Through our analysis of the prevalence of profanity in popular hip-hop and rap songs, we\n",
    "have analyzed various trends with multiple factors through data curation, parsing and\n",
    "management. We also performed exploratory data analysis and applied hypothesis testing in\n",
    "conjunction with machine learning to discover trends, correlations and outliers in the data and\n",
    "form subsequent conclusions regarding profanity in hip-hop.\n",
    "\n",
    "Our findings show that hip-hop and rap music has become undeniably more vulgar over\n",
    "time and the frequency of profane language in popular rap music has steadily increased. In one\n",
    "of our linear regression plots, we can identify the curve in the graph entitled ‘Swears Used Per\n",
    "Song vs Year’, where the average value depicting the average swears used per song\n",
    "represented by the slope almost doubles from 1985 to 2018.\n",
    "\n",
    "The increase in frequency of profane lyrics in hip-hop/rap can be attributed to an\n",
    "increase in exposure to profane rap music that correlates with its popularity. In other words, due\n",
    "to rap music rising in popularity and therefore being heard by more of the general population,\n",
    "more people are exposed to profanity and it gradually becomes the norm. As a result, it is no\n",
    "longer uncommon to hear profanity in popular rap lyrics, so an increase over time results as\n",
    "musicians continue to push that norm within the music industry."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
